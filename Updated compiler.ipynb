{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = {\"if\":\"if\",\"switch\":\"switch\",\"case\":\"case\",\"break\":\"break\",\"default\":\"default\",\"iterate\":\"iterate\",\n",
    "          \"in\":\"in\",\"range\":\"range\",\"while\":\"while\",\"elif\":\"elif\",\"else\":\"else\",\"class\":\"class\",\"return\":\"return\",\n",
    "          \"num\":\"DT\",\"dec\":\"DT\",\"alpha\":\"DT\",\"string\":\"DT\",\"flag\":\"DT\",\"1\":\"flag\",\"0\":\"flag\",\"func\":\"func\",\"safe\":\"safe\",\n",
    "          \"personal\":\"personal\",\"available\":\"available\",\"locked\":\"locked\",\"banned\":\"banned\",\"implicit\":\"implicit\", \n",
    "           \"stack\":\"stack\",\"overwritten\":\"overwritten\"}\n",
    "keyword[\"num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = {\"+\":\"PM\",\"-\":\"PM\",\"*\":\"MDM\",\"/\":\"MDM\",\"%\":\"MDM\",\"!\":\"NOT\",\"~\":\"NOT\",\"&&\":\"&&\",\"||\":\"||\",\"band\":\"band\",\"bor\":\"bor\",\n",
    "      \"<\":\"RO\",\">\":\"RO\",\"<=\":\"RO\",\">=\":\"RO\",\"!=\":\"RO\",\"==\":\"RO\",\"=\":\"Assign\",\"<<\":\"shiftOp\",\">>\":\"shiftOp\",\"+=\":\"AssignOp\",\n",
    "       \"*=\":\"AssignOp\",\"/=\":\"AssignOp\",\"%=\":\"AssignOp\",\"<<=\":\"AssignOp\",\">>=\":\"AssignOp\",\"++\":\"Inc_Dec\",\"--\":\"Inc_Dec\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = {\",\":\",\" , \";\":\";\" , \":\":\":\" , \".\":\".\" , \"{\":\"{\" , \"}\":\"}\" , \"(\":\"(\" , \")\":\")\" ,\n",
    "        \"[\":\"[\" , \"]\":\"]\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#tokenclass\n",
    "class token:\n",
    "    def __init__(self,text):\n",
    "        #text in temp\n",
    "        self.text=text\n",
    "    def tokenizer(self):\n",
    "        \n",
    "        if self.text in keyword:\n",
    "            \n",
    "            for key,value in keyword.items():\n",
    "            \n",
    "                if self.text ==key:#y is class part and a is value part\n",
    "                    if value==self.text:\n",
    "                        y=value\n",
    "                        a=\" \"\n",
    "                    else:\n",
    "                        \n",
    "                        y=value\n",
    "                        a = self.text\n",
    "        elif self.text in punc:\n",
    "            \n",
    "             for key,value in punc.items():\n",
    "                    \n",
    "                    if self.text ==key:\n",
    "                        if value==self.text:\n",
    "                                y=value\n",
    "                                a=\" \"\n",
    "                        else:\n",
    "                        \n",
    "                            y=value\n",
    "                            a = self.text\n",
    "                      \n",
    "        elif self.text in opr:\n",
    "            \n",
    "            for key,value in opr.items():\n",
    "                  if self.text ==key:\n",
    "                        if value==self.text:\n",
    "                                y=value\n",
    "                                a=\" \"\n",
    "                        else:\n",
    "                        \n",
    "                            y=value\n",
    "                            a = self.text\n",
    "                      \n",
    "                      \n",
    "        elif self.text == '\\n':\n",
    "             \n",
    "            y =self.text\n",
    "            a =\" \"\n",
    "        elif re.match(r'^[+-]?[0-9]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"INTEGER CONSTANT\"\n",
    "        elif re.match(r'^[+-]?[0-9]*(.)[0-9]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"FLOAT CONSTANT\"\n",
    "        elif re.match(r'^_+[a-zA-Z_0-9]*[A-Za-z0-9]$|^[A-Za-z]+[A-Za-z_0-9]*[A-Za-z0-9]$|^[A-Za-z]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"IDENTIFIER\" \n",
    "            \n",
    "        elif re.match(r\"^'[a-zA-Z0-9]'$|^'[!@#$%^&*()-=+{}|;:<>,.?/']'$|^'\\\\[\\'\\\"\\\\]'$|^'\\\\[ntrafb0]'$\",self.text):\n",
    "            a=self.text\n",
    "            y=\"CHAR CONSTANT\"\n",
    "        elif re.match(\"^\\\"([a-zA-Z0-9]|(\\\\\\\\\\\\\\\\)|[!@#$%^&*()-=+{}|;:<>,.?/']|\\\\[|\\\\]|_|\\\\\\\\[nrtfab0]|\\\\\\\\\\\"|\\\\\\\\')*\\\"$\",self.text):\n",
    "            a=self.text\n",
    "            y=\"STRING CONSTANT\"\n",
    "            \n",
    "        else:\n",
    "            y=\"Invalid Token\"\n",
    "            a=self.text\n",
    "    \n",
    "        \n",
    "        return y,a\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SAF.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"SAF1.txt\", 'w') as f:\n",
    "    for line in lines:\n",
    "        # Keep the Shebang line\n",
    "        if line[0:2] == \"#!\":\n",
    "            f.writelines(line)\n",
    "        # Also keep existing empty lines\n",
    "        elif not line.strip():\n",
    "            f.writelines(line)\n",
    "        # But remove comments from other lines\n",
    "        else:\n",
    "            line = line.split('#')\n",
    "            stripped_string = line[0].rstrip()\n",
    "            # Write the line only if the comment was after the code.\n",
    "            # Discard lines that only contain comments.\n",
    "            if stripped_string:\n",
    "                f.writelines(stripped_string)\n",
    "                f.writelines('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Invalid Token', '{{', 0]\n",
      "['{', ' ', 0]\n",
      "['\\n', ' ', 0]\n",
      "['while', ' ', 1]\n",
      "['(', ' ', 1]\n",
      "['IDENTIFIER', 'a', 1]\n",
      "['.', ' ', 1]\n",
      "['IDENTIFIER', 'b', 1]\n",
      "['.', ' ', 1]\n",
      "['IDENTIFIER', 'c', 1]\n",
      "['.', ' ', 1]\n",
      "['IDENTIFIER', 'd', 1]\n",
      "['RO', '!=', 1]\n",
      "['Assign', '=', 1]\n",
      "['INTEGER CONSTANT', '75', 1]\n",
      "['AssignOp', '+=', 1]\n",
      "['Assign', '=', 1]\n",
      "['Invalid Token', '&&&b', 1]\n",
      "['.', ' ', 1]\n",
      "['IDENTIFIER', 'x', 1]\n",
      "['.', ' ', 1]\n",
      "['Invalid Token', '7xd', 1]\n",
      "['\\n', ' ', 1]\n",
      "['IDENTIFIER', 'a', 2]\n",
      "['shiftOp', '<<', 2]\n",
      "['RO', '<', 2]\n",
      "['RO', '<=', 2]\n",
      "['RO', '==', 2]\n",
      "['Assign', '=', 2]\n",
      "['IDENTIFIER', 'b', 2]\n",
      "['Invalid Token', '!!', 2]\n",
      "['RO', '!=', 2]\n",
      "['RO', '==', 2]\n",
      "['Assign', '=', 2]\n",
      "['INTEGER CONSTANT', '65', 2]\n",
      "['.', ' ', 2]\n",
      "['INTEGER CONSTANT', '75', 2]\n",
      "['.', ' ', 2]\n",
      "['IDENTIFIER', 'ab', 2]\n",
      "['.', ' ', 2]\n",
      "['Invalid Token', '5c', 2]\n",
      "['.', ' ', 2]\n",
      "['INTEGER CONSTANT', '666', 2]\n",
      "['.', ' ', 2]\n",
      "['INTEGER CONSTANT', '786', 2]\n",
      "['\\n', ' ', 2]\n",
      "['IDENTIFIER', 'a', 3]\n",
      "['AssignOp', '+=', 3]\n",
      "['Assign', '=', 3]\n",
      "['Invalid Token', '**', 3]\n",
      "['MDM', '*', 3]\n",
      "['MDM', '*', 3]\n",
      "['Invalid Token', '//', 3]\n",
      "['MDM', '/', 3]\n",
      "['IDENTIFIER', 'a', 3]\n",
      "['Assign', '=', 3]\n",
      "['IDENTIFIER', 'b', 3]\n",
      "['Assign', '=', 3]\n",
      "['IDENTIFIER', 'c', 3]\n",
      "['\\n', ' ', 3]\n",
      "['DT', 'string', 4]\n",
      "['IDENTIFIER', 'str', 4]\n",
      "['AssignOp', '+=', 4]\n",
      "['Assign', '=', 4]\n",
      "['Invalid Token', '\"abcd', 4]\n",
      "['AssignOp', '*=', 4]\n",
      "['RO', '==', 4]\n",
      "['Assign', '=', 4]\n",
      "['Invalid Token', '\\\\\\\\\\\\\"abcd', 4]\n",
      "['AssignOp', '+=', 4]\n",
      "['Assign', '=', 4]\n",
      "['Invalid Token', '56\"', 4]\n",
      "['PM', '+', 4]\n",
      "['INTEGER CONSTANT', '5', 4]\n",
      "['\\n', ' ', 4]\n",
      "['DT', 'alpha', 5]\n",
      "['IDENTIFIER', 'ch', 5]\n",
      "['Assign', '=', 5]\n",
      "['Invalid Token', \"\\\\\\\\\\\\n'\", 5]\n",
      "['PM', '+', 5]\n",
      "['Invalid Token', \"'\", 5]\n",
      "['AssignOp', '+=', 5]\n",
      "['Assign', '=', 5]\n",
      "['CHAR CONSTANT', \"'\\\\n'\", 5]\n",
      "['PM', '+', 5]\n",
      "['Invalid Token', \"''''r\", 5]\n",
      "['\\n', ' ', 5]\n",
      "['Invalid Token', '', 6]\n",
      "['\\n', ' ', 6]\n",
      "END OF FILE\n",
      "[['Invalid Token', '{{', 0], ['{', ' ', 0], ['\\n', ' ', 0], ['while', ' ', 1], ['(', ' ', 1], ['IDENTIFIER', 'a', 1], ['.', ' ', 1], ['IDENTIFIER', 'b', 1], ['.', ' ', 1], ['IDENTIFIER', 'c', 1], ['.', ' ', 1], ['IDENTIFIER', 'd', 1], ['RO', '!=', 1], ['Assign', '=', 1], ['INTEGER CONSTANT', '75', 1], ['AssignOp', '+=', 1], ['Assign', '=', 1], ['Invalid Token', '&&&b', 1], ['.', ' ', 1], ['IDENTIFIER', 'x', 1], ['.', ' ', 1], ['Invalid Token', '7xd', 1], ['\\n', ' ', 1], ['IDENTIFIER', 'a', 2], ['shiftOp', '<<', 2], ['RO', '<', 2], ['RO', '<=', 2], ['RO', '==', 2], ['Assign', '=', 2], ['IDENTIFIER', 'b', 2], ['Invalid Token', '!!', 2], ['RO', '!=', 2], ['RO', '==', 2], ['Assign', '=', 2], ['INTEGER CONSTANT', '65', 2], ['.', ' ', 2], ['INTEGER CONSTANT', '75', 2], ['.', ' ', 2], ['IDENTIFIER', 'ab', 2], ['.', ' ', 2], ['Invalid Token', '5c', 2], ['.', ' ', 2], ['INTEGER CONSTANT', '666', 2], ['.', ' ', 2], ['INTEGER CONSTANT', '786', 2], ['\\n', ' ', 2], ['IDENTIFIER', 'a', 3], ['AssignOp', '+=', 3], ['Assign', '=', 3], ['Invalid Token', '**', 3], ['MDM', '*', 3], ['MDM', '*', 3], ['Invalid Token', '//', 3], ['MDM', '/', 3], ['IDENTIFIER', 'a', 3], ['Assign', '=', 3], ['IDENTIFIER', 'b', 3], ['Assign', '=', 3], ['IDENTIFIER', 'c', 3], ['\\n', ' ', 3], ['DT', 'string', 4], ['IDENTIFIER', 'str', 4], ['AssignOp', '+=', 4], ['Assign', '=', 4], ['Invalid Token', '\"abcd', 4], ['AssignOp', '*=', 4], ['RO', '==', 4], ['Assign', '=', 4], ['Invalid Token', '\\\\\\\\\\\\\"abcd', 4], ['AssignOp', '+=', 4], ['Assign', '=', 4], ['Invalid Token', '56\"', 4], ['PM', '+', 4], ['INTEGER CONSTANT', '5', 4], ['\\n', ' ', 4], ['DT', 'alpha', 5], ['IDENTIFIER', 'ch', 5], ['Assign', '=', 5], ['Invalid Token', \"\\\\\\\\\\\\n'\", 5], ['PM', '+', 5], ['Invalid Token', \"'\", 5], ['AssignOp', '+=', 5], ['Assign', '=', 5], ['CHAR CONSTANT', \"'\\\\n'\", 5], ['PM', '+', 5], ['Invalid Token', \"''''r\", 5], ['\\n', ' ', 5], ['Invalid Token', '', 6], ['\\n', ' ', 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#openning file\n",
    "with open(\"SAF1.txt\",\"r\")as f:\n",
    "    file=f.read()\n",
    "#empty token list\n",
    "all_tokens=[]\n",
    "#the one works correctly\n",
    "\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','|','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "symtk=[',','.',':','}','{','[',']','(',')']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "all_tokens = []\n",
    "le=''\n",
    "sp=' '\n",
    "c = 0\n",
    "key=op+sym+eq+symtk\n",
    "string=file\n",
    "for a,i in enumerate(string):\n",
    "    if i == '\\n':\n",
    "        \n",
    "        \n",
    "        le+=i\n",
    "        toke = token(le)\n",
    "        n=toke.tokenizer()\n",
    "        for t in n:\n",
    "            result.append(t)\n",
    "        result.append(c)\n",
    "        all_tokens.append(result)\n",
    "        print(result)\n",
    "        c+=1\n",
    "\n",
    "        result=[]\n",
    "        le=le.strip()\n",
    "        #c+=1\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if i !=sp and i not in sym:#checking and appending into le                  eq=[\"=\"]\n",
    "\n",
    "            if i in op and string[a+1] in eq :#double assignment\n",
    "                le=i+string[a+1]\n",
    "        \n",
    "            elif i == string[a+1] and i!=string[a-1] :\n",
    "            \n",
    "                  le=i+string[a+1]                             #double operater++\n",
    "        \n",
    "            elif i in eq and string[a-1] in op and i==string[a-1]:\n",
    "                pass\n",
    "           # elif i in op and string[a+1]in op:\n",
    "            #     le=i+string[a+1] \n",
    "            else:\n",
    "                 le+=i\n",
    "        \n",
    "    except IndexError:\n",
    "        print(\"END OF FILE\")\n",
    "        \n",
    " \n",
    "   \n",
    "    if(a+1<len(string)):\n",
    "        if string[a+1] == sp or string[a+1] in key or le in key  or string[a+1] == '\\n' :#checking the next word(breaker)\n",
    "            if le != '':\n",
    "                \n",
    "                #token for/n\n",
    "                if '\\n' in le:\n",
    "                    le=le.strip()\n",
    "                    #print(le.replace('\\n',''))\n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)    \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []             \n",
    "                    le=''\n",
    "                else:  \n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)     \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []               \n",
    "                    le=''\n",
    "                    \n",
    "            elif le == '' and string[a+1]==sp:\n",
    "                \n",
    "                \n",
    "                pass\n",
    "    \n",
    "    \n",
    "\n",
    "print(all_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IDENTIFIER', 'a', 0]\n",
      "['AssignOp', '+=', 0]\n",
      "['IDENTIFIER', 'b', 0]\n",
      "['\\n', ' ', 0]\n",
      "['IDENTIFIER', 'a', 1]\n",
      "['Inc_Dec', '++', 1]\n",
      "['IDENTIFIER', 'b', 1]\n",
      "['\\n', ' ', 1]\n",
      "['IDENTIFIER', 'a', 2]\n",
      "['AssignOp', '+=', 2]\n",
      "['RO', '==', 2]\n",
      "['IDENTIFIER', 'b', 2]\n",
      "['\\n', ' ', 2]\n",
      "['IDENTIFIER', 'a', 3]\n",
      "['RO', '==', 3]\n",
      "END OF FILE\n",
      "[['IDENTIFIER', 'a', 0], ['AssignOp', '+=', 0], ['IDENTIFIER', 'b', 0], ['\\n', ' ', 0], ['IDENTIFIER', 'a', 1], ['Inc_Dec', '++', 1], ['IDENTIFIER', 'b', 1], ['\\n', ' ', 1], ['IDENTIFIER', 'a', 2], ['AssignOp', '+=', 2], ['RO', '==', 2], ['IDENTIFIER', 'b', 2], ['\\n', ' ', 2], ['IDENTIFIER', 'a', 3], ['RO', '==', 3]]\n"
     ]
    }
   ],
   "source": [
    "#openning file\n",
    "with open(\"SAF.txt\",\"r\")as f:\n",
    "    file=f.read()\n",
    "#empty token list\n",
    "all_tokens=[]\n",
    "#the one works correctly\n",
    "\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','|','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "symtk=[',','.',':','}','{','[',']','(',')']\n",
    "result = []\n",
    "all_tokens = []\n",
    "le=''\n",
    "sp=' '\n",
    "c = 0\n",
    "key=op+sym+eq+symtk\n",
    "string=file\n",
    "for a,i in enumerate(string):\n",
    "    if i == '\\n':\n",
    "             \n",
    "        le+=i\n",
    "        toke = token(le)\n",
    "        n=toke.tokenizer()\n",
    "        for t in n:\n",
    "            result.append(t)\n",
    "        result.append(c)\n",
    "        all_tokens.append(result)\n",
    "        print(result)\n",
    "        c+=1\n",
    "\n",
    "        result=[]\n",
    "        le=le.strip()\n",
    "        #c+=1\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if i !=sp and i not in sym:#checking and appending into le                  eq=[\"=\"]\n",
    "\n",
    "            if i in op and string[a+1] in eq :#double assignmenttrue\n",
    "                le=i+string[a+1]\n",
    "            elif i in eq and string[a-1] in op :#+= not=\n",
    "                pass\n",
    "            elif i in op and string[a+1] == i :#double operator\n",
    "                le = i+string[a+1]\n",
    "            elif i in eq and string[a+1] == i:#==\n",
    "                le = i+string[a+1]\n",
    "            elif i in eq and string[a-1] == i:#== not =\n",
    "                pass\n",
    "            elif i in op and string[a-1] == i :#++ not +\n",
    "                pass\n",
    "            \n",
    "                    #double operater++\n",
    "        \n",
    "            \n",
    "            else:\n",
    "                 le+=i\n",
    "        \n",
    "    except IndexError:\n",
    "        print(\"END OF FILE\")\n",
    "        \n",
    " \n",
    "   \n",
    "    if(a+1<len(string)):\n",
    "        if string[a+1] == sp or string[a+1] in key or le in key  or string[a+1] == '\\n' :#checking the next word(breaker)\n",
    "            if le != '':\n",
    "                \n",
    "                #token for/n\n",
    "                if '\\n' in le:\n",
    "                    le=le.strip()\n",
    "                    #print(le.replace('\\n',''))\n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)    \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []             \n",
    "                    le=''\n",
    "                else:  \n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)     \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []               \n",
    "                    le=''\n",
    "                    \n",
    "            elif le == '' and string[a+1]==sp:\n",
    "                \n",
    "                \n",
    "                pass\n",
    "    \n",
    "#Comment +,-,/    \n",
    "\n",
    "print(all_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[['IDENTIFIER', 'a', 0], ['AssignOp', '+=', 0], ['IDENTIFIER', 'b', 0], ['\\\\n', ' ', 0], ['IDENTIFIER', 'a', 1], ['Inc_Dec', '++', 1], ['IDENTIFIER', 'b', 1], ['\\\\n', ' ', 1]]\"]\n"
     ]
    }
   ],
   "source": [
    "a=str(all_tokens)\n",
    "#writting the tokens in the file\n",
    "with open(\"tokens.txt\",'w')as tk:\n",
    "    tk.writelines(a)\n",
    "#reading\n",
    "with open(\"tokens.txt\",'r')as tk:\n",
    "    to=tk.readlines()\n",
    "print(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
